# -*- coding: utf-8 -*-
"""ID3_DecisionTree.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/188P38WO6b01_Q8P2yiiJJxy1FkI2_SBn
"""

## Name: Niharika Kumar
## Batch: A2
## Roll No: 19
                          ## Practical No. 06

## Aim: Write a program to implement decision tree.






import math
import pandas as pd

# Define the playtennis dataset
playtennis_data = {
    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain', 'Sunny', 'Overcast', 'Overcast', 'Rain'],
    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],
    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],
    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],
    'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']
}

# Create DataFrame
df = pd.DataFrame(playtennis_data)

# Display the DataFrame
print(df)


total_instances=14
count_yes = df['PlayTennis'].value_counts()['Yes']

count_no = df['PlayTennis'].value_counts()['No']


# Calculate the probabilities
prob_yes = count_yes / total_instances
prob_no = count_no / total_instances

# Calculate entropy
entropy_of_playtennis_data = -(prob_yes * math.log2(prob_yes)) - (prob_no * math.log2(prob_no))
print("Entropy of PlayTennis data:", entropy_of_playtennis_data)

## Calculating count

overcast_yes_count = df[(df['Outlook'] == 'Overcast') & (df['PlayTennis'] == 'Yes')].shape[0]
overcast_no_count = df[(df['Outlook'] == 'Overcast') & (df['PlayTennis'] == 'No')].shape[0]
sunny_yes_count = df[(df['Outlook'] == 'Sunny') & (df['PlayTennis'] == 'Yes')].shape[0]
sunny_no_count = df[(df['Outlook'] == 'Sunny') & (df['PlayTennis'] == 'No')].shape[0]
rain_yes_count = df[(df['Outlook'] == 'Rain') & (df['PlayTennis'] == 'Yes')].shape[0]
rain_no_count = df[(df['Outlook'] == 'Rain') & (df['PlayTennis'] == 'No')].shape[0]

weak_yes_count = df[(df['Wind'] == 'Weak') & (df['PlayTennis'] == 'Yes')].shape[0]
weak_no_count = df[(df['Wind'] == 'Weak') & (df['PlayTennis'] == 'No')].shape[0]
strong_yes_count = df[(df['Wind'] == 'Strong') & (df['PlayTennis'] == 'Yes')].shape[0]
strong_no_count = df[(df['Wind'] == 'Strong') & (df['PlayTennis'] == 'No')].shape[0]

hot_yes_count = df[(df['Temperature'] == 'Hot') & (df['PlayTennis'] == 'Yes')].shape[0]
hot_no_count = df[(df['Temperature'] == 'Hot') & (df['PlayTennis'] == 'No')].shape[0]
mild_yes_count = df[(df['Temperature'] == 'Mild') & (df['PlayTennis'] == 'Yes')].shape[0]
mild_no_count = df[(df['Temperature'] == 'Mild') & (df['PlayTennis'] == 'No')].shape[0]
cool_yes_count = df[(df['Temperature'] == 'Cool') & (df['PlayTennis'] == 'Yes')].shape[0]
cool_no_count = df[(df['Temperature'] == 'Cool') & (df['PlayTennis'] == 'No')].shape[0]

high_yes_count = df[(df['Humidity'] == 'High') & (df['PlayTennis'] == 'Yes')].shape[0]
high_no_count = df[(df['Humidity'] == 'High') & (df['PlayTennis'] == 'No')].shape[0]
normal_yes_count = df[(df['Humidity'] == 'Normal') & (df['PlayTennis'] == 'Yes')].shape[0]
normal_no_count = df[(df['Humidity'] == 'Normal') & (df['PlayTennis'] == 'No')].shape[0]

# Calculate entropies
entropy_sunny = -((sunny_yes_count/(sunny_yes_count+sunny_no_count))*math.log2(sunny_yes_count/(sunny_yes_count+sunny_no_count))) - ((sunny_no_count/(sunny_yes_count+sunny_no_count))*math.log2(sunny_no_count/(sunny_yes_count+sunny_no_count)))
entropy_overcast = 0  # Assuming entropy is 0 for overcast since it's a pure subset
entropy_rain = -((rain_yes_count/(rain_yes_count+rain_no_count))*math.log2(rain_yes_count/(rain_yes_count+rain_no_count))) - ((rain_no_count/(rain_yes_count+rain_no_count))*math.log2(rain_no_count/(rain_yes_count+rain_no_count)))

# Calculate information gain
info_gain_outlook = entropy_of_playtennis_data - (5/14)*entropy_sunny - entropy_overcast - (5/14)*entropy_rain

print("Information gain of Attribute Outlook:", info_gain_outlook)

entropy_weak = -(weak_yes_count/(weak_yes_count+weak_no_count))*math.log2(weak_yes_count/(weak_yes_count+weak_no_count)) - (weak_no_count/(weak_yes_count+weak_no_count))*math.log2(weak_no_count/(weak_yes_count+weak_no_count))
entropy_strong = -(strong_yes_count/(strong_yes_count+strong_no_count))*math.log2(strong_yes_count/(strong_yes_count+strong_no_count)) - (strong_no_count/(strong_yes_count+strong_no_count))*math.log2(strong_no_count/(strong_yes_count+strong_no_count))

entropy_hot = -(hot_yes_count/(hot_yes_count+hot_no_count))*math.log2(hot_yes_count/(hot_yes_count+hot_no_count)) - (hot_no_count/(hot_yes_count+hot_no_count))*math.log2(hot_no_count/(hot_yes_count+hot_no_count))
entropy_mild = -(mild_yes_count/(mild_yes_count+mild_no_count))*math.log2(mild_yes_count/(mild_yes_count+mild_no_count)) - (mild_no_count/(mild_yes_count+mild_no_count))*math.log2(mild_no_count/(mild_yes_count+mild_no_count))
entropy_cool = -(cool_yes_count/(cool_yes_count+cool_no_count))*math.log2(cool_yes_count/(cool_yes_count+cool_no_count)) - (cool_no_count/(cool_yes_count+cool_no_count))*math.log2(cool_no_count/(cool_yes_count+cool_no_count))

entropy_high = -(high_yes_count/(high_yes_count+high_no_count))*math.log2(high_yes_count/(high_yes_count+high_no_count)) - (high_no_count/(high_yes_count+high_no_count))*math.log2(high_no_count/(high_yes_count+high_no_count))
entropy_normal = -(normal_yes_count/(normal_yes_count+normal_no_count))*math.log2(normal_yes_count/(normal_yes_count+normal_no_count)) - (normal_no_count/(normal_yes_count+normal_no_count))*math.log2(normal_no_count/(normal_yes_count+normal_no_count))

# Calculate information gain
info_gain_wind = entropy_of_playtennis_data - ((8/14)*entropy_weak + (6/14)*entropy_strong)
info_gain_temperature = entropy_of_playtennis_data - ((4/14)*entropy_hot + (6/14)*entropy_mild + (4/14)*entropy_cool)
info_gain_humidity = entropy_of_playtennis_data - ((7/14)*entropy_high + (7/14)*entropy_normal)

print("Information gain of Attribute Wind:", info_gain_wind)
print("Information gain of Attribute Temperature:", info_gain_temperature)
print("Information gain of Attribute Humidity:", info_gain_humidity)

max_info_gain = max(info_gain_wind, info_gain_temperature, info_gain_humidity, info_gain_outlook)

# Determine the attribute with the maximum information gain
if max_info_gain == info_gain_wind:
    print("Root of decision tree: Wind")
elif max_info_gain == info_gain_temperature:
    print("Root of decision tree: Temperature")
elif max_info_gain == info_gain_humidity:
    print("Root of decision tree: Humidity")
else:
    print("Root of decision tree: Outlook")

from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import pandas as pd
import matplotlib.pyplot as plt

# Define the playtennis dataset
playtennis_data = {
    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain', 'Sunny', 'Overcast', 'Overcast', 'Rain'],
    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],
    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],
    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],
    'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']
}

# Create a DataFrame from the dataset
df = pd.DataFrame(playtennis_data)

# Encode categorical variables
label_encoders = {}
for column in df.columns:
    label_encoders[column] = LabelEncoder()
    df[column] = label_encoders[column].fit_transform(df[column])

# Split the dataset into features (X) and target variable (y)
X = df.drop(columns=['PlayTennis'])
y = df['PlayTennis']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train a Decision Tree classifier
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

# Evaluate the model
accuracy = clf.score(X_test, y_test)
print("Accuracy:", accuracy)

# Visualize the decision tree
plt.figure(figsize=(12, 8))
plot_tree(clf, feature_names=X.columns, class_names=label_encoders['PlayTennis'].classes_, filled=True)
plt.show()

##Implementing decision trees in Python offers a straightforward and interpretable approach to predictive modeling. It's user-friendly, flexible for handling various data types, and provides interpretable models. However, decision trees are prone to overfitting and might not scale well to large datasets. Techniques like pruning and ensemble methods can help mitigate these issues. Overall, decision trees are a valuable tool for building predictive models in Python, particularly for smaller to medium-sized datasets.